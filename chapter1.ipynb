{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 认识urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "urllib是python的标准库,它提供丰富的函数例如从web服务器请求数据、处理cookie等，在python2中对应urllib2库，不同于urllib2，python3的urllib被分为若干子模块：urllib.request、urllib.parse、urllib.error等，urllib库的使用可以参考https://docs.python.org/3/library/urllib.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "html = urlopen(\"http://pythonscraping.com/pages/page1.html\")\n",
    "print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 认识BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup库用于解析html文本，并转化为BeautifulSoup对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "bsObj = BeautifulSoup(html.read(),\"html.parser\")\n",
    "print(bsObj.h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup函数需要制定解析库，下表列出常见的几种解析库，并给出优缺点:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|解析库|使用方法|优势|劣势|\n",
    "|--|--|--|--|\n",
    "|Python标准库|BeautifulSoup(html,’html.parser’)|Python内置标准库；执行速度快|容错能力较差\n",
    "|lxml HTML解析库|BeautifulSoup(html,’lxml’)|速度快；容错能力强|需要安装，需要C语言库\n",
    "|lxml XML解析库|BeautifulSoup(html,\\[‘lxml’,’xml’\\])|速度快；容错能力强；支持XML格式|需要C语言库\n",
    "|htm5lib解析库|BeautifulSoup(html,’htm5llib’)|以浏览器方式解析，最好的容错性|速度慢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 可靠性爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道在网站访问中通常会出现404 Page not found的情况，或者服务器暂时关闭了，在调用urlopen函数时就会抛出异常，使得程序无法继续运行，我们可以urllib.error模块来处理异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urlopen error [Errno 111] Connection refused>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen(\"https://www.baid.com/\")   #url is wrong\n",
    "except URLError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在取得可靠性连接后，我们用BeautifulSoup处理html，通常会出现网站改版后无法找到某个标签从而抛出异常的情形。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'li'\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "try:\n",
    "    bsObj = BeautifulSoup(html.read(),\"lxml\")\n",
    "    li = bsObj.ul.li\n",
    "    print(li)\n",
    "except AttributeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 第一个爬虫程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(),\"lxml\")\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    \n",
    "    return title\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found.\")\n",
    "else:\n",
    "    print(title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
